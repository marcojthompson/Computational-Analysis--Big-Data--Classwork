{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Getting data—scraping and APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "This week is about getting data from the big ol' Internet, with the Wikipedia as our guinea pig. The main task today is to retrieve the Wikipedia pages of **all Marvel characters** using the MediaWiki **API**. There are three parts to this exercise set.\n",
    "\n",
    "* Learn the basics of how to retrieve data from Wiki sites using the MediaWiki API\n",
    "* Download all Marvel character Wikipedia articles\n",
    "* Begin to explore the data\n",
    "\n",
    "With the data you acquire today, you will be working for the remainder of the semester. Try to get as far as possible, structure the data nicely and write your code so that it makes sense to you in the coming weeks.\n",
    "\n",
    "Also, there's an **important practice** you should start getting used to—which matters when we grade assignments. \n",
    "1. Openly reflect on how you solve a problem. It can be code comments, or markup below/above the code cell, just as long as you share your thoughts. \n",
    "2. Comment on your results, discussing:\n",
    "    * Whether they make sense\n",
    "    * If they look somewhat as you expected, and if not, what the reasons for this difference might be\n",
    "    * What—interesting or not—insight they reveal about the given system you analyze\n",
    "    \n",
    "    *Note: of course you can't always say something profound about every little thing, so rest assured, I will only expect explanations in your assignments when *it makes sense* that there should be one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use an API?** You could just go ahead and scrape the HTML from a Wikipedia page as simple as:\n",
    "\n",
    "    import requests as rq\n",
    "    rq.get(\"https://en.wikipedia.org/wiki/Batman\").text\n",
    "    \n",
    "Well... to navigate data in HTML format is not always easy. Therefore, MediaWiki offers its users direct use of its API. To load the MediaWiki markup using the API, one would do something like:\n",
    "\n",
    "    rq.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles=Batman&prop=revisions&rvprop=content\").json()\n",
    "    \n",
    "This assumes the data is JSON formatted and returns a `dict` object inside which you can find all sorts of information about the page, including the latest revision of the Batman page markup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful code to display `dict` object as a tree.** Have a look at it, make sure you understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T09:32:27.423821Z",
     "start_time": "2019-11-11T09:32:26.873226Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchcomplete : str - 0\n",
      "warnings \n",
      "    main \n",
      "        * : str - 287\n",
      "    revisions \n",
      "        * : str - 163\n",
      "query \n",
      "    pages \n",
      "        4335 \n",
      "            pageid : int - 4\n",
      "            ns : int - 1\n",
      "            title : str - 6\n",
      "            revisions : list - 180833\n"
     ]
    }
   ],
   "source": [
    "def print_dict_tree(d, indent=0):\n",
    "    \"\"\"Print tree of keys in `dict` object.\n",
    "    \n",
    "    Prints the different levels of nested keys in a `dict` object. When there\n",
    "    are no more dictionaries to key into, prints objects type and byte-size.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    d : dict\n",
    "    \"\"\"\n",
    "    for key, value in d.items():\n",
    "        print('    ' * indent + str(key), end=' ')\n",
    "        if isinstance(value, dict):\n",
    "            print(); print_dict_tree(value, indent+1)\n",
    "        else:\n",
    "            print(\":\", str(type(d[key])).split(\"'\")[1], \"-\", str(len(str(d[key]))))\n",
    "            \n",
    "# Example\n",
    "import requests as rq\n",
    "data = rq.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles=Batman&prop=revisions&rvprop=content\").json()\n",
    "print_dict_tree(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Learn to access Wikipedia data with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand how Wikipedia markup works. You'll need to know a bit about formatting of MediaWiki pages so that you can parse the markup that you retrieve from wikipedia. See http://www.mediawiki.org/wiki/Help:Formatting. In particular, look into how links work and how tables work and make sure you can answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.0.1**: How do you link to another Wikipedia page from within a Wikipedia-page, using the wikimedia markup? Write down a simple example that links to a specific section in another page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Answer to 3.0.1\n",
    "\n",
    "If we wanted to link to a specific page and create text with a hyperlink embedded, we would type [{webiste address} \"hyperlink text\"]. We can also use a bare link and wikipedia will recognize it as a link, or use the protocol-relative link [//{website addres(after //)} \"Hyperlink text\"], which will use the appropriate protocol. EX: [//en.wikipedia.org/wiki/United_States} United States] would lead to the wikipedia page on the United States.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.0.2**: What is the MediaWiki markup to create a simple table like the one below?\n",
    ">\n",
    ">| True Positive  | False Positive |\n",
    "| -------------- |:--------------:|\n",
    "| False Negative | True Negative  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Answer to 3.0.2\n",
    "\n",
    "{|\n",
    "| True Positive\n",
    "| False Positive\n",
    "|-\n",
    "| False Negative\n",
    "| True Negative\n",
    "|}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.0.3**: Figure out how to download pages from Wikipedia. Familiarize yourself with [the API](http://www.mediawiki.org/wiki/API:Main_page) (there's a nice little [tutorial](https://www.mediawiki.org/wiki/API:Tutorial), and further info about the [Query action](https://www.mediawiki.org/wiki/API:Query)) and learn how to extract the markup. The API query that returns the markup of the Batman page is:\n",
    ">\n",
    ">`https://www.wikipedia.org/w/api.php?format=json&action=query&titles=Batman&prop=revisions&rvprop=content`\n",
    ">\n",
    ">1. Explain the structure of this query. What are the parameters and arguments and what do they mean? What happens if you remove `rvprop=content`?\n",
    "2. Download the Batman page using the API and save it in a new variable. Extract the markup from the `dict` object and save it to a file called \"batman.txt\". We usually get hung up on this in class, so the first student to successfully extract the markup can share their solution with me so I can validate it and then share it with the class; s/he  gets **one extra credit point**!\n",
    ">\n",
    "> *Hint: 2. Use `print_dict_tree` to understand the hierarchy of keys and values in the data you get from the API. To extract the markup, you need to first key into 'query' then 'pages', and so on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Answer to 3.0.3.1\n",
    "\n",
    "The \"format\" parameter specifies how the response will be structured, in this case JSON. The \"action\" tag specifies the action to be taken, which is a query. The \"titles\" parameter is the title of the wikipedia page to be queried, in the case it's Batman. The \"prop\" parameter is a field that allows the user to specify what type of information abut the page the wish to retrieve, in the this case the list and number of revisions. The \"rvprop\" parameter allows the user to get specific parts of the visions, specifically the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "#Answer to 3.0.3.2\n",
    "import requests as rq\n",
    "\n",
    "#The following method was taken from a pastebin written by Michael DiSanto(not by me)\n",
    "def write_to_file(name):\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles={name}&prop=revisions&rvprop=content\"\n",
    "    data = rq.get(url).json()\n",
    "    \n",
    "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
    "    content = data[\"query\"][\"pages\"][page_id][\"revisions\"][0][\"*\"]\n",
    "    file = f\"{name}.txt\"\n",
    "    with open(file, \"w\") as f:\n",
    "        for line in content:\n",
    "            f.write(line)\n",
    "            \n",
    "\n",
    "try:\n",
    "    write_to_file(\"Batman\")\n",
    "    print(\"Success!\")\n",
    "except:\n",
    "    print(\"There was a problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Get data (main part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a good part of this course we will be working with data from Wikipedia. Today, your objective is to crawl a large dataset with good and bad characters from the Marvel universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 3.1.1**: From the Wikipedia API, get a list of all Marvel superheroes and another list of all Marvel supervillains. Use the `get_categorymembers` function below to get the characters in each category: 'Category:Marvel_Comics_supervillains' and 'Category:Marvel_Comics_superheroes'. Make sure you spend some time understanding the code.  How is the query formed?  Why does it take that form?  It will help to look at the [Categorymembers API](https://www.mediawiki.org/wiki/API:Categorymembers).  Moreoever, understand the need for the while loop and role played by the `cmcontinue` variable and query argument.\n",
    "\n",
    ">After you've obtained the lists for superheroes and supervillains, write some code to answer:\n",
    "1. How many characters are *ambiguous*, i.e. are both heroes and villains? What is the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) between the two groups?\n",
    "2. How many superheroes are there? How many supervillains? Do not include ambiguous characters in these counts!\n",
    ">\n",
    ">*Hint: Google something like \"get list all pages in category wikimedia api\" if you're struggling with the query. Also, you may notice that not only Marvel character pages are returned, but also names of subcategories. For now just ignore this and treat them as if they are also characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function that gets the categorymembers of a category\n",
    "def get_categorymembers(category):\n",
    "    members = []\n",
    "    cmcontinue = \"\"\n",
    "    while True:\n",
    "\n",
    "        # Download data\n",
    "        data = rq.get('https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=%s&cmlimit=max&cmcontinue=%s' % (category, cmcontinue)).json()    \n",
    "        #print(data)\n",
    "        \n",
    "        # Add member titles\n",
    "        members.extend(\n",
    "            [m['title'] for m in data['query']['categorymembers']]\n",
    "        )\n",
    "\n",
    "        # If there is a 'continue' key in `data` then fetch the next 'cmcontinue' value\n",
    "        if 'continue' in data:\n",
    "            cmcontinue = data['continue']['cmcontinue']\n",
    "\n",
    "        # Otherwise break\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 106 ambiguous characters.\n",
      "The Jaccard similarity between heroes and villans is 0.08709942481511915\n"
     ]
    }
   ],
   "source": [
    "#Answer 3.1.1.1\n",
    "\n",
    "#Use the get_categorymembers method to get superheros and villians. \n",
    "heroes = set(get_categorymembers('Category:Marvel_Comics_superheroes'))\n",
    "villains = set(get_categorymembers('Category:Marvel_Comics_supervillains'))\n",
    "\n",
    "#The Jaccard Similarity is the number of items in the intersection over the number of items in the union\n",
    "intersection_heroes_villains = set(heroes.intersection(villains))\n",
    "union_heroes_villans = set(heroes.union(villains))\n",
    "Jaccard_sim = len(intersection_heroes_villains) / len(union_heroes_villans)\n",
    "\n",
    "#Print out the answers\n",
    "print(\"There are \" + str(len(intersection_heroes_villains)) + \" ambiguous characters.\")\n",
    "print(\"The Jaccard similarity between heroes and villains is \" + str(Jaccard_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 456 unambiguous heroes in the MCU.\n",
      "There are 655 unambiguous villains in the MCU.\n"
     ]
    }
   ],
   "source": [
    "#Answer 3.1.1.2\n",
    "\n",
    "#Alter the lists of heroes and villans to not include ambiguous characters \n",
    "heroes_no_amb = [i for i in heroes if i not in intersection_heroes_villains]\n",
    "villains_no_amb = [j for j in villains if j not in intersection_heroes_villains]\n",
    "\n",
    "#Print out the number of heroes and villans, respectively\n",
    "print(\"There are \" + str(len(heroes_no_amb)) + \" unambiguous heroes in the MCU.\")\n",
    "print(\"There are \" + str(len(villains_no_amb)) + \" unambiguous villains in the MCU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 3.1.2**: Using these three lists you now want to download all data you can about each character and store it on your harddrive.\n",
    "* Create three folders in your working directory, one for *heroes*, one for *villains*, and one for *ambiguous*.\n",
    "* For each character, download the markup on their pages (just like you did for Batman in 3.0.3) and save in a new file in the corresponding hero/villain/ambiguous folder.  Use the character's name as the filename.\n",
    "* **Importantly** do not put ambiguous characters into the hero or villains folder!\n",
    ">\n",
    ">*Hint: Some of the characters have funky names. The first problem you may encounter is with encoding. To solve that you can call `.encode('utf-8')` on your markup string. Another problem you may encounter is that some characters have a slash in their names. You should just replace the slash with some other meaningful character.*\n",
    ">Once your code will start running, it will take some time to download the data and create the files (30-40 minutes on my computer).  You might wish you had a measure of progress while the code is running, something like a progress bar.  Look no further than `tqdm`.  Here's an [example](https://www.geeksforgeeks.org/python-how-to-make-a-terminal-progress-bar-using-tqdm/) how to download and use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.1**: Extract the length of the page of each character (to do so you will have to open the corresponding file) and plot the distribution of this variable for each class (heroes/villains/ambiguous). Can you say anything about the popularity of characters in the Marvel universe based on your visualization?\n",
    ">\n",
    ">*Hint: The simplest thing is to make a probability mass function, i.e. a normalized histogram. [My figure](https://github.com/lucian979/CarletonBD/blob/main/plots/ex3.2.1.pdf) looks like this. Use `plt.hist` on a list of page lengths, with the argument `density=True`. Other distribution plots are fine too, though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.2**: Find the 10 characters from each class with the longest Wikipedia pages. Visualize their page lengths with bar charts. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alliances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.3**: In this exercise you want to find out the biggest alliances in the Marvel universe and their members. The data that will help you in doing this is in the *alliances*-field of the markup of each character -- open up a couple of character files and look for that field; get a sense for how the information is stored so that you can then write code to retreave it. Below I suggest steps you can take to solve the problem if you get stuck.\n",
    "* Use the regular expression `alliances[\\w\\W]+?\\n` to extract the *alliances*-field of a character's markup.\n",
    "* Use the regular expression `\\[\\[.+?[\\]\\|]` to extract links (i.e. each team) from the *alliance*-field.\n",
    "* You want to store alliance names and the corresponding members (hint: use a `defaultdict`).\n",
    "* Inspect your team names. Are there any that result from inconsistencies in the information on the pages? How do you deal with this?\n",
    "* **Print the 10 largest alliances and their number of members.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of using regex\n",
    "import re\n",
    "\n",
    "for title in titles:\n",
    "        \n",
    "    # Need to replace / with - before loading file\n",
    "    title = title.replace(\"/\", \"-\")\n",
    "        \n",
    "    # Load character markup\n",
    "    with open(f\"../data/{folder}/{title}.txt\") as fp:\n",
    "        markup = fp.read()\n",
    "    \n",
    "    # Get alliance field\n",
    "    alliances_field = re.findall(r\"alliances[\\w\\W]+?\\n\", markup)\n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.4 EXTRA**: We are interested in knowing if there is a time-trend in the debut of characters.\n",
    "* Extract into three lists, debut years of heroes, villains, and ambiguous characters.\n",
    "* Do all pages have a debut year? Do some have multiple? How do you handle these inconsistencies?\n",
    "1. For heroes, villains and ambiguous character seperately, visualize the amount of characters introduced over time. You choose how you want to visualize this data, but please comment on your choice.\n",
    "2. Make a plot that shows what fraction of introduced characters each year are heros. Taken together, **comment on your visualizations** and what they say about the system you're analyzing.\n",
    ">\n",
    ">*Hint: The debut year is given on the debut row in the info table of a character's Wiki-page. There are many ways that you can extract this variable. You should try to have a go at it yourself, but if you are short on time, you can use this horribly ugly regular expression code:*<br><br>\n",
    "*`re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup_text)[0])[0][:-1]`*\n",
    ">\n",
    "> ***Will not be included in assignment. Worth up to 5 extra credit.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
