{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "The exercises today are about extracting high-level knowledge from text. We're still a long way from computers being able to give us insight as deep as that which we can aquire from manually reading text, but some the tools that you will use today get us a long way in understanding useful things about unreadibly large amount of text in comparatively little time. In the exercises today you will:\n",
    "\n",
    "* Create wordclouds\n",
    "* Extract sentiment from text\n",
    "* Construct a Bag of Words (BoW) matrix to represent how words are used about each faction in the Marvel dataset\n",
    "* Perform a TD-IDF transform to understand which words are important to different characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task when working with text, is almost always to somehow clean the text. In our case, text is formatted\n",
    "as Mediawiki markup, which has its quirks as you probably remember. So we need to clean this text and most of the\n",
    "way we need to do it manually. But while this is a meaningful thing to learn to do, we don't want to take time away from the more important exercises\n",
    "that come later in this exercise set. Therefore, you will find a helper function in this notebook that extracts and cleans the markup text of a Wikipedia page.  Remember to change the path so that it points to the data folder on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:39:22.277628Z",
     "start_time": "2020-01-28T13:39:21.487694Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def get_clean_text(title, folder):\n",
    "    \"\"\"Given a character name (title) and the faction it belongs to\n",
    "    return the page markup as a neatly cleaned string.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "        title : str\n",
    "        folder : str\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "        text : str\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "        >>> get_clean_text('Iron Man', 'heroes')\n",
    "    \"\"\"\n",
    "    # Load markup\n",
    "    with open(f\"../../data/{folder}/{title}.txt\") as fp:\n",
    "        text = fp.read()\n",
    "\n",
    "    # Remove category links\n",
    "    text = re.sub(r'\\[\\[Category.+\\]\\]', '', text)\n",
    "    \n",
    "    # Fix links (match and clear \"[[Iron Man (comic book)|\" and \"[[\", then on next line \"]]\"\n",
    "    text = re.sub(r'(\\[\\[((?!\\]\\]).)+\\|)|(\\[\\[)', '', text)\n",
    "    text = re.sub(r'\\]\\]', '', text)\n",
    "    \n",
    "    # Remove '''\n",
    "    text = re.sub(r\"'''\", '', text)\n",
    "    \n",
    "    # Remove refs\n",
    "    text = re.sub(r'<ref.+?</ref>', '', text)\n",
    "    \n",
    "    # Remove other ugly html links\n",
    "    text = re.sub(r'<.+?>', '', text)\n",
    "    \n",
    "    # Remove '=' signs in headers\n",
    "    text = re.sub(r'=+', '', text)\n",
    "    \n",
    "    # Remove table and external links\n",
    "    text = re.sub(r'\\{\\{[\\w\\W]*?\\}\\}', '', text)\n",
    "    \n",
    "    # Remove everything after \"See Also\"\n",
    "    text = re.sub(r'== ?See [aA]lso[\\w\\W]+', \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:39:22.277628Z",
     "start_time": "2020-01-28T13:39:21.487694Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "print(get_clean_text('Black Widow (Natasha Romanova)', 'heroes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although they probably offer more buzz than usefulness, wordclouds are a fun way to get quick insight into which words are used in a corpus of text. In this section you will generate some.  We start with an example generating a wordcloud for Iron Man.\n",
    "\n",
    "> *Hint: You must install `wordcloud` with anaconda by typing into your console*\n",
    ">\n",
    ">        conda install -c conda-forge wordcloud\n",
    ">\n",
    ">*or*\n",
    ">\n",
    ">        pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud example\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "ironman_text = get_clean_text(\"Iron Man\", \"heroes\")\n",
    "\n",
    "wc_ironman = WordCloud(max_font_size=40).generate(ironman_text)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(wc_ironman, interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.1.1**: Make a word cloud for each faction using the `wordcloud` module! Use `max_font_size=40` to make the wordcloud managable.\n",
    "\n",
    ">* For each faction, concatenate all (cleaned) text about each character into one long string.\n",
    ">* Make three plots with appropriate titles, so we can compare them visually.\n",
    ">* You could choose to exclude all character names from the strings, that my give more interesting results. Not a requirement.\n",
    ">* Comment on the differences you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have an enormous amount of text, and you want to know something about its level of negativity, neutrality or positivity. Enter *Sentiment Analysis*. The point of this exercise is to extract the sentiment of text on your heroes, villains and ambiguous characters and figure out whether Wikipedia is biased towards writing in a certain tone towards a certain kind of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.2.1**: Compute and visualize sentiment scores.\n",
    "* Using the `VADER` algorithm supported in `nltk` (i.e. `from nltk.sentiment.vader import SentimentIntensityAnalyzer`), you can obtain three scores for a piece of text: it's inferred negativity, neutrality and positivity.\n",
    "* Use `SentimentIntensityAnalyzer` to get these three scores for each character. Then, for each faction, make three distribution plots (e.g. histograms), that each show how negativity, neutrality and positivity are distributed, respectively. That's 9 plots in total.\n",
    "* In each of these plots, report the mean of the distribution. Your plot should look [something along the lines of this](https://dhsvendsen.github.io/images/sentiment_histograms.png).\n",
    "* Comment on your result. Does writing in Wikipedia seem biased to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Bag of Words matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, \"Bag of Words\" means breaking up a document into words and throwing them into a bag. And that's very close to the truth! In week 5 you constructed a \"team-affiliations\" matrix which had a row for each character and a column for each team. If the character was on a given team there would be a one for that character row at that team column, if not there would be a zero. The BoW is the same, only now, rather than teams, your columns are individual words that a character's wikipage might contain, and the numbers represent how many times those words appear.\n",
    "<img src=\"http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_bow.png\" width=\"400\"/>\n",
    "BoW's are pretty large and sparse (mostly contain zero's) matrices, but they are extremely useful because they allow us to use linear algebra to do things like PCA, classification, etc..\n",
    "\n",
    "Here is a function for extracting a cleaned list of words from a string of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T20:32:21.501566Z",
     "start_time": "2020-01-29T20:32:21.492498Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get list of stopwords and add a few\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_clean_words(text, exclude_words=[]):\n",
    "    \"\"\"Given some text, return a list of clean words.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "        text : str\n",
    "        exclude_words : list\n",
    "            Words to exclude (e.g. characters own name)\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "        words : list\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "        >>> my_text = get_clean_text('Iron Man', 'heroes')\n",
    "        >>> my_words = get_clean_words(my_text, exclude_words=['Iron', 'Man'])\n",
    "    \"\"\"\n",
    "\n",
    "    # Extarct words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Add exclude words to words to give it same treatment\n",
    "    words += exclude_words\n",
    "    \n",
    "    # Convert to lower case\n",
    "    words = [w.lower() for w in words]\n",
    "    \n",
    "    # Clear punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [w.translate(table) for w in words]\n",
    "    \n",
    "    # Seperate words and exclude words\n",
    "    if len(exclude_words) > 0:\n",
    "        exclude_words = words[-len(exclude_words):]\n",
    "        words = words[:-len(exclude_words)]\n",
    "    \n",
    "    # Remove non-alphabetic words\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [w for w in words if not w in stop_words | set(exclude_words)]\n",
    "    \n",
    "    # Remove single letter words \n",
    "    words = [w for w in words if len(w) > 1]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.3.1**: Create a bag of words matrix that contains all your characters. Maintain also a target array, so you know whether a row corresponds to a hero, a villain or an ambiguous character. Also, **do not include words that only appear for one character**.\n",
    ">*Hint: Since you already extracted lists of words for each faction in Ex. 7.1.1, you can use these to figure out what the total vocabulary of words used in your dataset is. You can \"clean up\" this vocabulary by a number of tricks. For example, there are tools for *stemming* words to remove grammar so that e.g. 'cat' and 'cats' both become 'cat', but that's all up to you whether you want to go that deep.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many different words are in your vocabulary/columns are in your matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the 10 most used words, and the 10 least used words, along with their usage count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the distribution of how many times words are used. Scale it appropriately. What does this distribution look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.3.2.EXTRA**: Fit a PCA to your BoW matrix. Make a bar plot that shows the explained variance ratio of the first 20 components. What fraction of the total variance do the first 20 components explain?\n",
    ">\n",
    "> ***Earn up to 5 extra credit points for solving this problem.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.3.3.EXTRA**: Create a classifier that predicts if a character is a hero or a villain (not ambiguous) from the words used on their page. Report its 10-fold cross validation accuracy. Comment on the result and compare this score with the one obtained in week 5 where you trained on team alliances.\n",
    ">\n",
    "> ***Earn up to 5 extra credits for solving this problem.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You didn't just make that BoW matrix to count simple things. We are interested in knowing how (or if) words are used differently across characters, and the best way to do that is to used something called a Term Frequency - Inverse Document Frequency (TF-IDF) transformation. You can read more about it [on Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), but the gist is that it reflects how important each word is to each document (a document in this case being a character) in the chorpus.\n",
    "\n",
    "It works in two steps:\n",
    "* (1 - TF) you normalize over the frequency of each word in each document, so that rows sum to 1. Every row is now a probability distribution that gives the \"term frequency\" in each document.\n",
    "* (2 - IDF) you weigh the TF by the inverse document frequency, which measures how unique a word to specific documents. For example, the word \"the\" will be frequently used in every document (high TF) but we know it's not very special because it's used in all documents so the inverse document frequency is low, yielding a vanishing TF-IDF score for \"the\" in all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 7.4.1**: Use any tool you like (you can do it manually, it's straight forward if you understand the method), to perform a TF-IDF transform on your BoW matrix from Ex. 7.3.1. The result should be a matrix of the same shape as the BoW, but with different values inside.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain what these values mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For the top three most written about characters in each class (so 9 in total), print out each of their 10 highest scoring words. Comment on any differences you observe in the type of words being used in different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.4.2**: Install `umap-learn` and import it like `import umap`. UMAP is an algorithm for non-linear dimensionality reduction, that will allow you to visualize your data on a low-dimensional manifold. Using UMAP, transform the TF-IDF transformed BoW matrix into a 2-dimensional space. Plot the points in this space, colored by faction. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
